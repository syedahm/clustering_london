{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The city of London was built in the year 43 AD by the Roman empire and is a city rich in history and geographical diversity. It has seen wars, plagues, fires and most recently a virus pandemic. As the capital city of the United Kingdom, it is home to 9 million residents and is a global hub for commerce and culture. Each year the city attracts tens of millions of visitors who come to marvel at historic sites such the Tower of London and Buckingham palace. The population of the city continues to grow each year as it attracts many migrants from around the world, in search of employment and a cosmopolitan lifestyle. \n",
    "However, for all its charm the city has a fundamental problem. There is a severe shortage of housing in the city, and current supply is unable to keep up with the growth in demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem I aim to solve in this analysis is to identify the more affordable neighbourhoods that offer the same local venues and services as others. This will help anyone who lives in the city, or is looking to move to the city, find an affordable neighbourhood. Given the rapid rate of change in London, this will help those living in the city keep up with the changes and find affordable accommodation without compromising on their standard of living. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis I will cluster London neighbourhoods based their local features. The feature dataset will be obtained using the foursquare API service to extract a list of the venues nearby. This dataset will be used to group areas by their geographical similarity. I will also bring in data on house price transactions in the local area to understand local house prices and compare this for the different neighbourhoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests, io\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = os.environ.get('FOURSQUARE_ID')\n",
    "CLIENT_SECRET = os.environ.get('FOURSQUARE_SECRET')\n",
    "\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_url = 'https://www.doogal.co.uk/UKPostcodesCSV.ashx?area=London'\n",
    "london_codes_all = pd.read_csv(postcode_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_codes_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_codes_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data data obtained in this table contains a huge amount of information that will be useful for further analysis. At the moment we will only take the colums relevant to the geographical grouping of neighbourhoods and their positional coordinates as this is what we will use to extract the venue feature datset using foursqaure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_relevant_columns = london_codes_all[['District','Ward', 'Constituency', 'Postcode district', 'Postcode', 'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_relevant_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_relevant_columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table we can see that there are a number of different ways to group neighbourhoods in London; District, ward, consituency and postcode area. In addition we can see that there is an additonal level of granularity we can see in specific postcodes. We will visualise each of these grouping layers using folium maps library to understand the merits of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central coordinates for London obtained from google maps\n",
    "latitude = 51.5074\n",
    "longitude = -0.1278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(frame, layer):\n",
    "# create map using latitude and longitude values\n",
    "    map_folium = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "    # add markers to map\n",
    "    for lat, lng, layer in zip(frame['Latitude'], frame['Longitude'], frame[layer]):\n",
    "        label = '{}'.format(layer)\n",
    "        label = folium.Popup(label, parse_html=True)\n",
    "        folium.CircleMarker(\n",
    "            [lat, lng],\n",
    "            radius=5,\n",
    "            popup=label,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='#3186cc',\n",
    "            fill_opacity=0.7,\n",
    "            parse_html=False).add_to(map_folium)  \n",
    "\n",
    "    return map_folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a centroid location for the different layers of geographical groupings, I have used the group by method and will take the mean of the components coordinates that form a geographical layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consituencies = london_relevant_columns.groupby('Constituency').agg('mean').reset_index()\n",
    "consituencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map(consituencies,'Constituency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Borough = london_relevant_columns.groupby('District').agg('mean').reset_index()\n",
    "create_map(Borough,'District')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = london_relevant_columns.groupby('Postcode district').agg('mean').reset_index()\n",
    "create_map(districts,'Postcode district')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map above shows us that using postocode districts results in large geographical cluster of points in central London. these points much closer to each other than the points in the outer areas of the city. As a result any analysis on geographical features may result in a skewed data set for these points are they will be very geographically similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to correct this, we can either use a higher layer geographical aggregation or we will have to clean the data in some way to reduce these clusters points. We can  use the code below to clean the data at Postcode district level. However, I will revisit this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residential london only\n",
    "districts = districts[\n",
    "    (districts['Postcode district'].str[0]=='E') \n",
    "    | (districts['Postcode district'].str[0]=='N')\n",
    "    | (districts['Postcode district'].str[0]=='S')\n",
    "    | (districts['Postcode district'].str[0]=='W')\n",
    "]\n",
    "districts = districts[\n",
    "    (districts['Postcode district'].str[0:2]!='EN')\n",
    "    & (districts['Postcode district'].str[0:2]!='SM')\n",
    "    & (districts['Postcode district'].str[0:2]!='WD')\n",
    "    & (districts['Postcode district'].str[0:2]!='WC')\n",
    "    & (districts['Postcode district'].str[0:2]!='W1')\n",
    "    & (districts['Postcode district'].str[0:2]!='EC')\n",
    "    & (districts['Postcode district'].str[0:3]!='SW1')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse data on UK house transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The url below  provides us with data on house purchases in 2020. This is detailed at a transaction level for each property and includes the prices paid for the property and the date of the transaction. This data is obtained from the land registry office of HM government. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_2020_url = 'http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020.csv'\n",
    "prices_2020 = pd.read_csv(prices_2020_url, header=None)\n",
    "prices_2020.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices_2020.rename(columns={\n",
    "#     0:'id',\n",
    "#     1:'price',\n",
    "#     2:'transaction_date',\n",
    "#     3:'postcode',\n",
    "    \n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the preview of the dataframe above that this dataset does not include any headers and includes transations from all parts of the UK. The header names can be interpreted due to knowledge of the data, and those familiar with UK addresses will recognise that this data includes all the expected fields such as postcode, city, address etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this analysis is concerned with London neighbourhoods, I will filter the dataset below for only london transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_prices = prices_2020[prices_2020[13]=='GREATER LONDON']\n",
    "london_prices.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction data is remerged here with the original data set including positional data for each postcode and geographical layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_geo = london_prices.merge(london_relevant_columns, how='left', left_on=3, right_on='Postcode')\n",
    "transactions_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_geo[transactions_geo['Postcode'].notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transactions_geo.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transactional data alongside the geographical layers that correspond to them, we can use this to analyse the distribution of house prices across the different geographical layers of looking at London addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_transactions(frame, layer):\n",
    "    avg_prices = frame[[layer,1]].groupby(layer).mean().reset_index()\n",
    "    avg_prices[1] = avg_prices[1].astype(int)\n",
    "    avg_prices.rename(columns={1:'avg_price'}, inplace=True)\n",
    "    return avg_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hit_price(frame):\n",
    "    bins = np.linspace(frame['avg_price'].min(), frame['avg_price'].max(), 25)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(frame.columns[0])\n",
    "    return plt.hist(frame['avg_price'], bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution by postcode\n",
    "postcode_prices = group_transactions(transactions_geo, 'Postcode district')\n",
    "plot_hit_price(postcode_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "constituency_prices = group_transactions(transactions_geo, 'Constituency')\n",
    "# bins = np.linspace(y.min(), y.max(), 25)\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.hist(constituency_prices['avg_price'], bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hit_price(constituency_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_prices = group_transactions(transactions_geo, 'District')\n",
    "plot_hit_price(borough_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the charts above that the distribution of house prices follows a fairly normal distribution, but then has long tail skewing the data towards higher values. This is no surprise given the high premium that is placed on prime central london real estate. In order to help group this data better, I will classify the prices into four bands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_classificaton(frame):\n",
    "    lower = np.percentile(frame['avg_price'],25)\n",
    "    median = np.percentile(frame['avg_price'],50)\n",
    "    upper = np.percentile(frame['avg_price'],75)\n",
    "    frame.loc[frame['avg_price'] > upper, 'price_band'] = 'expensive' \n",
    "    frame.loc[frame['avg_price'] <= upper, 'price_band'] = 'premium' \n",
    "    frame.loc[frame['avg_price'] <= median, 'price_band'] = 'mid range' \n",
    "    frame.loc[frame['avg_price'] <= lower, 'price_band'] = 'cheap' \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_prices = price_classificaton(group_transactions(transactions_geo, 'Postcode district'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_prices = price_classificaton(group_transactions(transactions_geo, 'Constituency'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use foursquare to get local venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wil extract neighbourhood features at two geographical layers, postcode area and constituency. I will repeat the analysis for both to understand what effect if any the geographical proximity of central london postcodes may have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# london_venues_raw = getNearbyVenues(names=districts['Postcode district'],\n",
    "#                                    latitudes=districts['Latitude'],\n",
    "#                                    longitudes=districts['Longitude']\n",
    "#                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# london_venues_raw.to_csv('london_venues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues_raw = pd.read_csv('london_venues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # consituencies\n",
    "# consitutency_venues_raw = getNearbyVenues(names=consituencies['Constituency'],\n",
    "#                                    latitudes=consituencies['Latitude'],\n",
    "#                                    longitudes=consituencies['Longitude']\n",
    "#                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consitutency_venues_raw.to_csv('constituency_venues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues = london_venues_raw.copy()\n",
    "london_venues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "london_venues['Venue Category'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(london_venues['Venue Category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above there are a lot of features for this dataset (385). This may affect the performance of the clustering algorithm and we may need to carry out some feature engineering to improve this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding postcode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(frame):\n",
    "    london_one_hot = pd.get_dummies(frame[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "    london_one_hot['layer'] = frame['Neighborhood']\n",
    "    london_grouped_category = london_one_hot.groupby('layer').mean().reset_index()\n",
    "    return london_grouped_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_grouped = pre_processing(london_venues)\n",
    "london_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "london_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary of top venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_venues(frame):\n",
    "    num_top_venues = 10\n",
    "\n",
    "    indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "    # create columns according to number of top venues\n",
    "    columns = ['layer']\n",
    "    for ind in np.arange(num_top_venues):\n",
    "        try:\n",
    "            columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "        except:\n",
    "            columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "    # create a new dataframe\n",
    "    top_venues = pd.DataFrame(columns=columns)\n",
    "    top_venues['layer'] = frame['layer']\n",
    "\n",
    "    for ind in np.arange(frame.shape[0]):\n",
    "        top_venues.iloc[ind, 1:] = return_most_common_venues(frame.iloc[ind, :], num_top_venues)\n",
    "        \n",
    "    return top_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted = get_top_venues(london_grouped)\n",
    "neighborhoods_venues_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering postcode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(frame):\n",
    "    elbow_df = frame.drop('layer', 1)\n",
    "#     elbow_df = frame\n",
    "    distortions = []\n",
    "    K = range(1,20)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k, random_state=0).fit(elbow_df)\n",
    "        distortions.append(sum(np.min(cdist(elbow_df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / elbow_df.shape[0])\n",
    "\n",
    "    # Plot the elbow\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('optimal k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def kmeans_plot(frame,k):\n",
    "    hist_plot_df = frame.drop('layer', 1)\n",
    "#     hist_plot_df = frame\n",
    "    # run k-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(hist_plot_df)\n",
    "    plt.title(f'k={k}')\n",
    "    # plot\n",
    "    return plt.hist(kmeans.labels_, bins=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(london_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows us that there is no obvious optimal point for K, and additional clusters continue to improve the clustering algorithm. This may be due to the fact that there very high number of features in this dataset. I will revisit this once the initial analysis is complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help further in identifying an optimal k, I will plot the frequency of each cluster below. This will give an insight into how successful the clustering approach has been in finding similarities between neighbourhoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since adding additonal clusters continues to improve accuracy I will use k = 7 for my analysis, as the historgram plots above indicate that this gives us an additional cluster with subtantial data points, indicating the algorithm has found additional similarities between clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tableau_postcodes = london_grouped.insert(0, 'Cluster Labels', labels)\n",
    "# london_grouped.to_csv('postcodes_tableau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "grouped_clustering = london_grouped.drop('layer', 1)\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(grouped_clustering)\n",
    "labels = kmeans.labels_\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', labels)\n",
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_map = districts.merge(neighborhoods_venues_sorted, left_on='Postcode district', right_on='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of london clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_map(frame, k):\n",
    "    # create map\n",
    "    map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "    # set color scheme for the clusters\n",
    "    x = np.arange(k)\n",
    "    ys = [i + x + (i*x)**2 for i in range(k)]\n",
    "    colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "    # add markers to the map\n",
    "    markers_colors = []\n",
    "    for lat, lon, poi, cluster in zip(frame['Latitude'], frame['Longitude'], frame['layer'], frame['Cluster Labels']):\n",
    "        label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "        folium.CircleMarker(\n",
    "            [lat, lon],\n",
    "            radius=5,\n",
    "            popup=label,\n",
    "            color=rainbow[cluster-1],\n",
    "            fill=True,\n",
    "            fill_color=rainbow[cluster-1],\n",
    "            fill_opacity=0.7).add_to(map_clusters)\n",
    "\n",
    "    return map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map(postcode_map,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the map above that the kmeans algorithm has been able to group certain areas of London based on their geographical features. We can look at the most common types of venues in those areas in those neighbourhoods to understand their common features. I will also join the average house price data back onto this data frame so we can get an idea of price bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_df(frame, cluster):\n",
    "    return frame.loc[frame['Cluster Labels'] == cluster, frame.columns[[0] + list(range(4, frame.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices = postcode_map.merge(postcode_prices, how='left', on='Postcode district')\n",
    "postcode_cluster_prices.shape\n",
    "# postcode_cluster_prices.to_csv('postcodes_tableau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == i, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[column].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top5_venue_counts = pd.DataFrame(columns=['cluster', 'venue', 'count'])\n",
    "count_list = []\n",
    "for i in range(7):\n",
    "    x = pd.DataFrame(columns=['cluster', 'venue', 'count'])\n",
    "#     count_list.append(\n",
    "#     postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == i, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head()\n",
    "#     )\n",
    "    x['cluster'] = 'cluster {}'.format(i)\n",
    "    x['venue'] = postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == i, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head().index\n",
    "    x['count'] = postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == i, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head()[0]\n",
    "    df_top5_venue_counts = pd.concat([df_top5_venue_counts,x])\n",
    "    df_top5_venue_counts = df_top5_venue_counts.drop('cluster', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_top5_venue_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top5_venue_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == 5, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == 6, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head()\n",
    "# postcode_cluster_prices.stack().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeating the analysis excluding prime central london real estate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues = london_venues[\n",
    "    (london_venues['Neighborhood'].str[0]=='E') \n",
    "    | (london_venues['Neighborhood'].str[0]=='N')\n",
    "    | (london_venues['Neighborhood'].str[0]=='S')\n",
    "    | (london_venues['Neighborhood'].str[0]=='W')\n",
    "]\n",
    "london_venues = london_venues[\n",
    "    (london_venues['Neighborhood'].str[0:2]!='EN')\n",
    "    & (london_venues['Neighborhood'].str[0:2]!='SM')\n",
    "    & (london_venues['Neighborhood'].str[0:2]!='WD')\n",
    "    & (london_venues['Neighborhood'].str[0:2]!='WC')\n",
    "    & (london_venues['Neighborhood'].str[0:2]!='EC')\n",
    "]\n",
    "london_venues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_grouped = pre_processing(london_venues)\n",
    "london_grouped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary of top venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted = get_top_venues(london_grouped)\n",
    "neighborhoods_venues_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(london_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "grouped_clustering = london_grouped.drop('layer', 1)\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(grouped_clustering)\n",
    "labels = kmeans.labels_\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', labels)\n",
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_map = districts.merge(neighborhoods_venues_sorted, left_on='Postcode district', right_on='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_map(postcode_map,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices = postcode_map.merge(postcode_prices, how='left', on='Postcode district')\n",
    "postcode_cluster_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering before preprocessing to improve clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method charts from above showed us that we cannot find an optimal value for K as the there was no significant elbow point which marked the best value. This is likely due to the fact the dataset currently has a large number of features (385 including outer london and central london postcodes). As we can see from features below, many of these features are very similar and differ due to slight name changes or sub categories. For example, the various different restraunts by world cuisine could be grouped together as restraunts. In this section I will carry out some feature engineering to reduce the number of features by combining these categories, to improve the clustering algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "london_venues['Venue Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(london_venues['Venue Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restaurant_msk = london_venues['Venue Category'].str.contains('Restaurant')\n",
    "Bar_msk = london_venues['Venue Category'].str.contains('Bar')\n",
    "Shop_msk = london_venues['Venue Category'].str.contains('Shop')\n",
    "Store_msk = london_venues['Venue Category'].str.contains('Store')\n",
    "Gym_msk = london_venues['Venue Category'].str.contains('Gym')\n",
    "food_place_msk = london_venues['Venue Category'].str.contains('Place')\n",
    "museum_msk = london_venues['Venue Category'].str.contains('Museum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(london_venues.loc[Shop_msk,'Venue Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(frame):\n",
    "    Restaurant_msk = frame['Venue Category'].str.contains('Restaurant')\n",
    "    Bar_msk = frame['Venue Category'].str.contains('Bar')\n",
    "    Shop_msk = frame['Venue Category'].str.contains('Shop')\n",
    "    Store_msk = frame['Venue Category'].str.contains('Store')\n",
    "    Gym_msk = frame['Venue Category'].str.contains('Gym')\n",
    "    food_place_msk = frame['Venue Category'].str.contains('Place')\n",
    "    museum_msk = frame['Venue Category'].str.contains('Museum')\n",
    "    frame.loc[Restaurant_msk, 'Venue Category'] = 'Restaurant'\n",
    "    frame.loc[Bar_msk, 'Venue Category'] = 'Bar'\n",
    "    frame.loc[Shop_msk, 'Venue Category'] = 'Shop'\n",
    "    frame.loc[Store_msk, 'Venue Category'] = 'Store'\n",
    "    frame.loc[Gym_msk, 'Venue Category'] = 'Gym'\n",
    "    frame.loc[food_place_msk, 'Venue Category'] = 'Place'\n",
    "    frame.loc[museum_msk, 'Venue Category'] = 'Museum'\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_venues = feature_engineering(london_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(london_venues['Venue Category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this, the feature engineering has halved the number of features in the dataset, without losing valuable information. Hopefully this will improve clustering. The approach will now be repeated with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating clustering with feature engineered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_grouped = pre_processing(london_venues)\n",
    "london_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted = get_top_venues(london_grouped)\n",
    "neighborhoods_venues_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(london_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(london_grouped, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "grouped_clustering = london_grouped.drop('layer', 1)\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(grouped_clustering)\n",
    "labels = kmeans.labels_\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', labels)\n",
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_map = districts.merge(neighborhoods_venues_sorted, left_on='Postcode district', right_on='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_map(postcode_map,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices = postcode_map.merge(postcode_prices, how='left', on='Postcode district')\n",
    "postcode_cluster_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(postcode_cluster_prices,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_cluster_prices.loc[postcode_cluster_prices['Cluster Labels'] == 1, postcode_cluster_prices.columns[list(range(4, postcode_cluster_prices.shape[1]))]].stack().value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering has greatly improved the clustering, we can identify clear characteristics of each cluster group distinc and unique from the others. The addition of the price data also allows us to identify the relative price of similar neighbourhoods side by side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans at constituency level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add information on london zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(frame):\n",
    "    london_one_hot = pd.get_dummies(frame[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "    london_one_hot['layer'] = frame['Neighborhood']\n",
    "    london_grouped_category = london_one_hot.groupby('layer').mean().reset_index()\n",
    "    return london_grouped_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constits = consitutency_venues_raw.copy()\n",
    "constits = pd.read_csv('constituency_venues.csv')\n",
    "constits = feature_engineering(constits)\n",
    "constits_one_hot = pre_processing(constits)\n",
    "elbow_method(constits_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(constits_one_hot,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(constits_one_hot,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_venues_sorted = get_top_venues(constits_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "constits_one_hot = constits_one_hot.drop('layer', 1)\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(constits_one_hot)\n",
    "labels = kmeans.labels_\n",
    "constit_venues_sorted.insert(0, 'Cluster Labels', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs_map_df = consituencies.merge(constit_venues_sorted, left_on='Constituency', right_on='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map(boroughs_map_df,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_cluster_prices = boroughs_map_df.merge(constituency_prices, how='left', on='Constituency')\n",
    "borough_cluster_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(borough_cluster_prices,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(borough_cluster_prices,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(borough_cluster_prices,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(borough_cluster_prices,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_df(borough_cluster_prices,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis above shows that using a higher level geography does not provide us with good separation of clusters. There are two reasons for this, 1) the foursquare API extract less feature data due to the broader geographies 2) the higher level geography covers such a diverse range of features that the algorithm is unable to distinguish features amongst them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans at consituency level with zonal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(frame):\n",
    "    x1 = pd.get_dummies(frame['London zone'])\n",
    "    x2 = pd.get_dummies(frame['Venue Category'])\n",
    "    london_one_hot = pd.concat([x1,x2], axis=1)\n",
    "    london_one_hot['layer'] = frame['Neighborhood']\n",
    "    london_grouped_category = london_one_hot.groupby('layer').mean().reset_index()\n",
    "    return london_grouped_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_relevant_columns = london_codes_all[['District','Ward', 'Constituency', 'Postcode district', 'Postcode', 'London zone', 'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consituencies = london_relevant_columns.groupby('Constituency').agg({'Latitude':'mean','Longitude':'mean', 'London zone':'median'}).reset_index()\n",
    "consituencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constits = consitutency_venues_raw.copy()\n",
    "constits = pd.read_csv('constituency_venues.csv')\n",
    "constits = constits.merge(consituencies[['Constituency','London zone']], how='left', left_on='Neighborhood', right_on='Constituency')\n",
    "constits = constits.drop('Constituency', 1)\n",
    "constits_one_hot = pre_processing(constits)\n",
    "elbow_method(constits_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_venues_sorted = get_top_venues(constits_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(constits_one_hot,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "constits_one_hot = constits_one_hot.drop('layer', 1)\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(constits_one_hot)\n",
    "labels = kmeans.labels_\n",
    "constit_venues_sorted.insert(0, 'Cluster Labels', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boroughs_map_df = consituencies.merge(constit_venues_sorted, left_on='Constituency', right_on='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map(boroughs_map_df,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df(boroughs_map_df,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans at constituency level with price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(frame):\n",
    "    london_one_hot = pd.get_dummies(frame[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "    london_one_hot['layer'] = frame['Neighborhood']\n",
    "    london_grouped_category = london_one_hot.groupby('layer').sum().reset_index()\n",
    "    return london_grouped_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_price(frame):\n",
    "    london_one_hot = pd.get_dummies(frame[['price_band']], prefix=\"\", prefix_sep=\"\")\n",
    "    london_one_hot['layer'] = frame['Constituency']\n",
    "    return london_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_prices = price_classificaton(group_transactions(transactions_geo, 'Constituency'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constits = consitutency_venues_raw.copy()\n",
    "constits_one_hot = pre_processing(constits)\n",
    "price_one_hot = pre_processing_price(constituency_prices)\n",
    "\n",
    "factor = 1\n",
    "columns = price_one_hot['layer']\n",
    "price_one_hot = price_one_hot[['cheap', 'expensive', 'mid range', 'premium']]/factor\n",
    "price_one_hot['layer'] = columns\n",
    "price_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_cluster = price_one_hot.merge(constits_one_hot, how='inner', on='layer')\n",
    "constit_venues_sorted = get_top_venues(constits_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_cluster = price_cluster.drop('layer', 1)\n",
    "elbow_method(price_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(price_cluster,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(price_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "len(labels)\n",
    "constit_venues_sorted.insert(0, 'Cluster Labels', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constit_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rejoin_clusters(consituencies,constit_venues_sorted)\n",
    "constituency_prices.rename(columns={'Constituency':'layer'}, inplace=True)\n",
    "final_df = x.merge(constituency_prices, on='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kcluster_price)\n",
    "ys = [i + x + (i*x)**2 for i in range(kcluster_price)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, band, cluster in zip(final_df['Latitude'], final_df['Longitude'], final_df['layer'], final_df['price_band'], final_df['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) +'-' + str(band) + ' ' + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['Cluster Labels'] == 0, final_df.columns[[0] + list(range(4, final_df.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['Cluster Labels'] == 1, final_df.columns[[0] + list(range(4, final_df.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['Cluster Labels'] == 2, final_df.columns[[0] + list(range(4, final_df.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['Cluster Labels'] == 3, final_df.columns[[0] + list(range(4, final_df.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['Cluster Labels'] == 4, final_df.columns[[0] + list(range(4, final_df.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df['Cluster Labels'] == 5, final_df.columns[[0] + list(range(4, final_df.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate how local venues may affect house prices I need to merge the prices data (numerical) with venue data (categorical) and evaluate this together in a classification model. The data must be pre processed and normalised, however there is a step of normalisatoin already in place for the categorical data where i have grouped by the mean of frequency of each occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# london_grouped_for_price = london_grouped.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the feature data set by summing the total to obtain the frequency of occurence, so this can be used with the preprocessing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_grouped_for_price = london_one_hot.groupby('Postcode district').sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtaining target prediction variable for the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_venue = avg_prices.merge(london_grouped_for_price[['Postcode district']], how='right', on='Postcode district')\n",
    "y = prices_venue['avg_price']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = london_grouped_for_price.copy().drop('Postcode district', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.replace([np.inf, -np.inf], np.nan)\n",
    "# X[X.isna().any(axis=1)]\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using PCA to reduce elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = PCA(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "# y = y.reshape(-1, 1)\n",
    "# y = preprocessing.StandardScaler().fit(y).transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(X, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 4\n",
    "# neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\n",
    "# yhat = neigh.predict(X_test)\n",
    "# print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\n",
    "# print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 4\n",
    "# neigh = KMeans(k).fit_transform(X_train)\n",
    "# yhat = neigh.predict(X_test)\n",
    "# print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\n",
    "# print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ks = 20\n",
    "# mean_acc = np.zeros((Ks-1))\n",
    "# std_acc = np.zeros((Ks-1))\n",
    "# ConfustionMx = [];\n",
    "# for n in range(1,Ks):\n",
    "    \n",
    "#     #Train Model and Predict  \n",
    "#     neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "#     yhat=neigh.predict(X_test)\n",
    "#     mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "\n",
    "    \n",
    "#     std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "# plt.plot(range(1,Ks),mean_acc,'g')\n",
    "# plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "# plt.legend(('Accuracy ', '+/- 3xstd'))\n",
    "# plt.ylabel('Accuracy ')\n",
    "# plt.xlabel('(K)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = london_grouped_for_price.copy().drop('Postcode district', 1)\n",
    "prices_venue = avg_prices.merge(london_grouped_for_price[['Postcode district']], how='right', on='Postcode district')\n",
    "y = prices_venue['avg_price']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import linear_model\n",
    "# regr = linear_model.LinearRegression()\n",
    "# x = np.asanyarray(X)\n",
    "# y = np.asanyarray(y)\n",
    "# regr.fit (x, y)\n",
    "# # The coefficients\n",
    "# print ('Coefficients: ', regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rejoining clusters with price data to see if this has any indication of value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find correlation for numeric variables\n",
    "\n",
    "target = prices_venue['avg_price']\n",
    "\n",
    "corr = train.corr()\n",
    "corr_abs = corr.abs()\n",
    "\n",
    "nr_num_cols = len(num_feat)\n",
    "\n",
    "ser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n",
    "print(ser_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest value Frequency percentage in categorical variables \n",
    "for i in list(cat_feat):\n",
    "    pct = df[i].value_counts()[0] / 2919\n",
    "    print('Highest value Percentage of {}: {:3f}'.format(i, pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest value Frequency percentage in categorical variables \n",
    "for i in list(cat_feat):\n",
    "    pct = df[i].value_counts()[0] / 2919\n",
    "    print('Highest value Percentage of {}: {:3f}'.format(i, pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
